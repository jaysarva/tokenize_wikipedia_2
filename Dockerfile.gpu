# GPU-enabled Dockerfile for Wikipedia tokenization and embedding generation
# Uses PyTorch with CUDA support for GPU-accelerated sentence embeddings

FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    ca-certificates \
    bash \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements-gpu.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements-gpu.txt

# Pre-download the MiniLM model to bake it into the image (optional but faster startup)
RUN python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"

# Copy application code
COPY . .

# Default command (can be overridden by RayJob entrypoint)
CMD ["python", "-m", "ray_app.embed_wiki", "--help"]

