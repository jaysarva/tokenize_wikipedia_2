# RayJob for processing Wikipedia CirrusSearch dumps
#
# This job downloads CirrusSearch dump files (if not already present) and
# tokenizes all articles using the streaming reader. Much faster than Live API
# mode since there are no rate limits.
#
# Prerequisites:
#   1. Create the dumps PVC: kubectl apply -n wiki-tokenizer -f k8s/pvc-dumps.yaml
#   2. Create the output PVC: kubectl apply -n wiki-tokenizer -f k8s/pvc.yaml
#
# Usage:
#   kubectl apply -n wiki-tokenizer -f k8s/rayjob-cirrus.yaml
#
# Monitor:
#   kubectl get rayjob wiki-cirrus -n wiki-tokenizer -w
#   kubectl logs -n wiki-tokenizer -l ray.io/node-type=head -f
#
# The init container downloads the dump file (~20GB) on first run.
# Subsequent runs skip the download if the file already exists.

apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: wiki-cirrus
  namespace: wiki-tokenizer
spec:
  entrypoint: >
    python -m ray_app.tokenize_wiki
    --cirrus-dir /dumps
    --output /output/token_counts.jsonl
    --tokens-dir /output/tokens
    --checkpoint /output/checkpoint.json
    --concurrency 8
  runtimeEnvYAML: |
    env_vars:
      TIKTOKEN_ENCODING: "cl100k_base"
  shutdownAfterJobFinishes: true
  ttlSecondsAfterFinished: 3600
  rayClusterSpec:
    rayVersion: "2.9.3"
    headGroupSpec:
      serviceType: ClusterIP
      rayStartParams:
        dashboard-host: "0.0.0.0"
        dashboard-port: "8265"
        dashboard-agent-listen-port: "52365"
        object-store-memory: "536870912"  # 512MB for larger workloads
      template:
        spec:
          # Init container downloads CirrusSearch dump if not present
          # Using alpine with curl (runs as root to handle NFS permissions)
          initContainers:
            - name: download-dumps
              image: alpine:3.19
              command: ["/bin/sh", "-c"]
              env:
                # Change this to use different wikis:
                #   enwiki     = English Wikipedia (~40GB, 6M+ articles)
                #   simplewiki = Simple English Wikipedia (~600MB, 200K articles)
                #   enwikibooks = English Wikibooks (~2GB, 100K articles)
                - name: WIKI_NAME
                  value: "simplewiki"
              args:
                - |
                  set -e
                  apk add --no-cache curl gzip
                  
                  # Use WIKI_NAME env var (default to simplewiki for testing)
                  WIKI="${WIKI_NAME:-simplewiki}"
                  echo "Target wiki: $WIKI"
                  
                  DUMP_BASE_URL="https://dumps.wikimedia.org/other/cirrussearch"
                  MARKER_SUFFIX=".complete"
                  
                  # Function to verify gzip file integrity (full verification, ~3-7 min for 40GB)
                  verify_gzip_full() {
                    local file="$1"
                    echo "Running full gzip integrity check (this may take a few minutes)..."
                    if gzip -t "$file" 2>/dev/null; then
                      echo "✓ Full integrity check passed"
                      return 0
                    else
                      echo "✗ File is corrupted or incomplete"
                      return 1
                    fi
                  }
                  
                  # Function to quick-check file size against expected (seconds, not minutes)
                  # Returns: 0=size matches, 1=size mismatch or can't determine
                  quick_size_check() {
                    local file="$1"
                    local url="$2"
                    
                    local local_size=$(stat -c%s "$file" 2>/dev/null || stat -f%z "$file" 2>/dev/null)
                    local remote_size=$(curl -sI -L "$url" | grep -i content-length | tail -1 | awk '{print $2}' | tr -d '\r')
                    
                    if [ -z "$local_size" ] || [ -z "$remote_size" ]; then
                      echo "Could not determine sizes for comparison"
                      return 1
                    fi
                    
                    echo "Local size:  $local_size bytes"
                    echo "Remote size: $remote_size bytes"
                    
                    if [ "$local_size" = "$remote_size" ]; then
                      echo "✓ Size matches expected"
                      return 0
                    else
                      echo "✗ Size mismatch (incomplete download)"
                      return 1
                    fi
                  }
                  
                  # Check for existing dump files for this wiki
                  for f in /dumps/${WIKI}-*-cirrussearch-content.json.gz; do
                    if [ -f "$f" ]; then
                      # Case 1: Has marker file - already verified, skip immediately
                      if [ -f "${f}${MARKER_SUFFIX}" ]; then
                        echo "Found verified dump file (has marker): $f"
                        ls -lh "$f"
                        exit 0
                      fi
                      
                      # Case 2: No marker - need to verify
                      echo "Found existing file without marker: $f"
                      echo "Will verify after determining remote URL..."
                      # Store for later verification
                      EXISTING_FILE="$f"
                    fi
                  done
                  
                  # Get the remote URL (needed for size check and potential download)
                  echo "Finding latest dump date..."
                  LATEST_DATE=$(curl -sL "$DUMP_BASE_URL/" | grep -oE '20[0-9]{6}' | sort -r | head -1)
                  if [ -z "$LATEST_DATE" ]; then
                    echo "ERROR: Could not determine latest dump date"
                    exit 1
                  fi
                  echo "Latest dump date: $LATEST_DATE"
                  
                  INDEX_URL="$DUMP_BASE_URL/$LATEST_DATE/"
                  # Find the content dump file for our wiki
                  DUMP_FILE=$(curl -sL "$INDEX_URL" | grep -oE "${WIKI}-[0-9]+-cirrussearch-content\.json\.gz" | head -1)
                  
                  if [ -z "$DUMP_FILE" ]; then
                    echo "ERROR: Could not find dump file in index"
                    exit 1
                  fi
                  REMOTE_URL="$INDEX_URL$DUMP_FILE"
                  echo "Remote file: $DUMP_FILE"
                  
                  # Now verify existing file if we found one
                  if [ -n "$EXISTING_FILE" ]; then
                    echo ""
                    echo "Verifying existing file: $EXISTING_FILE"
                    
                    # Quick size check first (takes seconds)
                    if quick_size_check "$EXISTING_FILE" "$REMOTE_URL"; then
                      # Size matches - do full gzip verification
                      if verify_gzip_full "$EXISTING_FILE"; then
                        echo "✓ File is valid! Creating marker for future runs..."
                        touch "${EXISTING_FILE}${MARKER_SUFFIX}"
                        ls -lh "$EXISTING_FILE"
                        exit 0
                      else
                        echo "Size matched but content corrupted - will re-download"
                        rm -f "$EXISTING_FILE"
                      fi
                    else
                      echo "Size mismatch - will resume download"
                      # Don't delete - curl -C - will resume
                    fi
                  fi
                  
                  # Proceed with download
                  echo ""
                  echo "Downloading from $REMOTE_URL..."
                  echo "This may take 30-60 minutes depending on network speed (~40GB)"
                  
                  # Download with resume support (-C -) and retry on failure
                  MAX_RETRIES=3
                  RETRY=0
                  while [ $RETRY -lt $MAX_RETRIES ]; do
                    if curl -L -C - --retry 3 --retry-delay 5 -o "/dumps/$DUMP_FILE" "$REMOTE_URL"; then
                      echo "Download completed, verifying integrity..."
                      if verify_gzip_full "/dumps/$DUMP_FILE"; then
                        # Create marker file to indicate successful download
                        touch "/dumps/${DUMP_FILE}${MARKER_SUFFIX}"
                        echo "Download complete and verified!"
                        break
                      else
                        echo "File verification failed, removing corrupted file..."
                        rm -f "/dumps/$DUMP_FILE"
                        RETRY=$((RETRY + 1))
                        if [ $RETRY -lt $MAX_RETRIES ]; then
                          echo "Retrying download (attempt $((RETRY + 1))/$MAX_RETRIES)..."
                          sleep 10
                        fi
                      fi
                    else
                      RETRY=$((RETRY + 1))
                      if [ $RETRY -lt $MAX_RETRIES ]; then
                        echo "Download failed, retrying (attempt $((RETRY + 1))/$MAX_RETRIES)..."
                        sleep 10
                      fi
                    fi
                  done
                  
                  if [ $RETRY -ge $MAX_RETRIES ]; then
                    echo "ERROR: Failed to download after $MAX_RETRIES attempts"
                    exit 1
                  fi
                  
                  ls -lh /dumps/
                  
                  # Fix permissions so Ray containers can read the file
                  chmod 644 /dumps/*.json.gz
              securityContext:
                runAsUser: 0
                runAsGroup: 0
              volumeMounts:
                - name: wiki-dumps
                  mountPath: /dumps
              resources:
                requests:
                  cpu: "100m"
                  memory: "256Mi"
                limits:
                  cpu: "1"
                  memory: "512Mi"
          containers:
            - name: ray-head
              image: "docker.io/jaysarva027/tokenize-wiki:v1"
              imagePullPolicy: Always
              resources:
                requests:
                  cpu: "2"
                  memory: "4Gi"
                limits:
                  cpu: "4"
                  memory: "8Gi"
              env:
                - name: RAY_memory_usage_threshold
                  value: "0.95"
              volumeMounts:
                - name: wiki-dumps
                  mountPath: /dumps
                  readOnly: true
                - name: wiki-output
                  mountPath: /output
          volumes:
            - name: wiki-dumps
              persistentVolumeClaim:
                claimName: wiki-dumps
            - name: wiki-output
              persistentVolumeClaim:
                claimName: wiki-output
    workerGroupSpecs:
      - groupName: cirrus-workers
        # Scale workers based on cluster size
        # Each worker can handle multiple concurrent tokenization tasks
        replicas: 4
        minReplicas: 2
        maxReplicas: 8
        rayStartParams:
          object-store-memory: "536870912"  # 512MB
        template:
          spec:
            containers:
              - name: ray-worker
                image: "docker.io/jaysarva027/tokenize-wiki:v1"
                imagePullPolicy: Always
                resources:
                  requests:
                    cpu: "2"
                    memory: "4Gi"
                  limits:
                    cpu: "4"
                    memory: "8Gi"
                env:
                  - name: RAY_memory_usage_threshold
                    value: "0.95"
                volumeMounts:
                  - name: wiki-dumps
                    mountPath: /dumps
                    readOnly: true
                  - name: wiki-output
                    mountPath: /output
            volumes:
              - name: wiki-dumps
                persistentVolumeClaim:
                  claimName: wiki-dumps
              - name: wiki-output
                persistentVolumeClaim:
                  claimName: wiki-output

